Notebooks de PySPark creados en Microsoft Fabric sobre datos alojados en Lakehouse para diversas tareas:

1) Carga y procesamiento de datos:
    - Lectura de datos desde diversas fuentes como Azure Data Lake Storage, SQL Databases, y otros.
    - Limpieza Y transformación de datos para eliminar valores nulos, duplicados, y realizar imputaciones.
  
2) Análisis exploratorio de datos (EDA):
    - Visualización de datos con bibliotecas como Matplotlib y Seaborn.
    - Estadísticas descriptivas para entender la distribución y características de los datos.
  
3) Preparación de datos para machine learning:
   
    - Feature engineering para crear nuevas características a partir de las existentes.
    - Normalización y escalado de datos para preparar los datos para algoritmos de machine learning.
    - División de datos en conjuntos de entrenamiento y prueba.
   
 4) Entrenamiento de modelos de machine learning:

    - Uso de bibliotecas de Spark para entrenar modelos de regresión, clasificación, clustering, redes neuronales, etc.
    - Evaluación de modelos utilizando métricas como precisión, recall, F1-score, etc.
    - Optimización de hiperparámetros para mejorar el rendimiento del modelo

5) Implementación y monitoreo:

    - Despliegue de modelos en producción
    - Monitoreo de modelos para asegurar que sigan funcionando correctamente y realizar ajustes si es necesario.
  
6) Automatización y orquestación:
     
    - Creación de pipelines de datos y machine learning para automatizar el flujo de trabajo.
